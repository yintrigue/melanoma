{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_demo_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wgCjXyay1_Ng","colab_type":"code","colab":{}},"source":["# USAGE\n","# python train.py\n","\n","# set the matplotlib backend so figures can be saved in the background\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","\n","# import the necessary packages\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import SGD\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.metrics import classification_report\n","from pyimagesearch.minivggnet import MiniVGGNet\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def csv_image_generator(inputPath, bs, lb, mode=\"train\", aug=None):\n","\t# open the CSV file for reading\n","\tf = open(inputPath, \"r\")\n","\n","\t# loop indefinitely\n","\twhile True:\n","\t\t# initialize our batches of images and labels\n","\t\timages = []\n","\t\tlabels = []\n","\n","\t\t# keep looping until we reach our batch size\n","\t\twhile len(images) < bs:\n","\t\t\t# attempt to read the next line of the CSV file\n","\t\t\tline = f.readline()\n","\n","\t\t\t# check to see if the line is empty, indicating we have\n","\t\t\t# reached the end of the file\n","\t\t\tif line == \"\":\n","\t\t\t\t# reset the file pointer to the beginning of the file\n","\t\t\t\t# and re-read the line\n","\t\t\t\tf.seek(0)\n","\t\t\t\tline = f.readline()\n","\n","\t\t\t\t# if we are evaluating we should now break from our\n","\t\t\t\t# loop to ensure we don't continue to fill up the\n","\t\t\t\t# batch from samples at the beginning of the file\n","\t\t\t\tif mode == \"eval\":\n","\t\t\t\t\tbreak\n","\n","\t\t\t# extract the label and construct the image\n","\t\t\tline = line.strip().split(\",\")\n","\t\t\tlabel = line[0]\n","\t\t\timage = np.array([int(x) for x in line[1:]], dtype=\"float32\")\n","\t\t\timage = image.reshape((64, 64, 3))\n","\n","\t\t\t# update our corresponding batches lists\n","\t\t\timages.append(image)\n","\t\t\tlabels.append(label)\n","\n","\t\t# one-hot encode the labels\n","\t\tlabels = lb.transform(np.array(labels))\n","\n","\t\t# if the data augmentation object is not None, apply it\n","\t\tif aug is not None:\n","\t\t\t(images, labels) = next(aug.flow(np.array(images),\n","\t\t\t\tlabels, batch_size=bs))\n","\n","\t\t# yield the batch to the calling function\n","\t\tyield (np.array(images), labels)\n","\n","# initialize the paths to our training and testing CSV files\n","TRAIN_CSV = \"flowers17_training.csv\"\n","TEST_CSV = \"flowers17_testing.csv\"\n","\n","# initialize the number of epochs to train for and batch size\n","NUM_EPOCHS = 75\n","BS = 32\n","\n","# initialize the total number of training and testing image\n","NUM_TRAIN_IMAGES = 0\n","NUM_TEST_IMAGES = 0\n","\n","# open the training CSV file, then initialize the unique set of class\n","# labels in the dataset along with the testing labels\n","f = open(TRAIN_CSV, \"r\")\n","labels = set()\n","testLabels = []\n","\n","# loop over all rows of the CSV file\n","for line in f:\n","\t# extract the class label, update the labels list, and increment\n","\t# the total number of training images\n","\tlabel = line.strip().split(\",\")[0]\n","\tlabels.add(label)\n","\tNUM_TRAIN_IMAGES += 1\n","\n","# close the training CSV file and open the testing CSV file\n","f.close()\n","f = open(TEST_CSV, \"r\")\n","\n","# loop over the lines in the testing file\n","for line in f:\n","\t# extract the class label, update the test labels list, and\n","\t# increment the total number of testing images\n","\tlabel = line.strip().split(\",\")[0]\n","\ttestLabels.append(label)\n","\tNUM_TEST_IMAGES += 1\n","\n","# close the testing CSV file\n","f.close()\n","\n","# create the label binarizer for one-hot encoding labels, then encode\n","# the testing labels\n","lb = LabelBinarizer()\n","lb.fit(list(labels))\n","testLabels = lb.transform(testLabels)\n","\n","# construct the training image generator for data augmentation\n","aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n","\twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n","\thorizontal_flip=True, fill_mode=\"nearest\")\n","\n","# initialize both the training and testing image generators\n","trainGen = csv_image_generator(TRAIN_CSV, BS, lb,\n","\tmode=\"train\", aug=aug)\n","testGen = csv_image_generator(TEST_CSV, BS, lb,\n","\tmode=\"train\", aug=None)\n","\n","# initialize our Keras model and compile it\n","model = MiniVGGNet.build(64, 64, 3, len(lb.classes_))\n","opt = SGD(lr=1e-2, momentum=0.9, decay=1e-2 / NUM_EPOCHS)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n","\tmetrics=[\"accuracy\"])\n","\n","# train the network\n","print(\"[INFO] training w/ generator...\")\n","H = model.fit(\n","\tx=trainGen,\n","\tsteps_per_epoch=NUM_TRAIN_IMAGES // BS,\n","\tvalidation_data=testGen,\n","\tvalidation_steps=NUM_TEST_IMAGES // BS,\n","\tepochs=NUM_EPOCHS)\n","\n","# re-initialize our testing data generator, this time for evaluating\n","testGen = csv_image_generator(TEST_CSV, BS, lb,\n","\tmode=\"eval\", aug=None)\n","\n","# make predictions on the testing images, finding the index of the\n","# label with the corresponding largest predicted probability\n","predIdxs = model.predict(x=testGen, steps=(NUM_TEST_IMAGES // BS) + 1)\n","predIdxs = np.argmax(predIdxs, axis=1)\n","\n","# show a nicely formatted classification report\n","print(\"[INFO] evaluating network...\")\n","print(classification_report(testLabels.argmax(axis=1), predIdxs,\n","\ttarget_names=lb.classes_))\n","\n","# plot the training loss and accuracy\n","N = NUM_EPOCHS\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n","plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n","plt.title(\"Training Loss and Accuracy on Dataset\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"lower left\")\n","plt.savefig(\"plot.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MwiBoT5f_h3x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1594339169857,"user_tz":420,"elapsed":361605,"user":{"displayName":"Tim Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMh4iph3mxnXQ-zbxwzekj-ovpu-c0cAOBzt1b=s64","userId":"17136390159455304905"}},"outputId":"844f34b5-baae-4c1e-b9f9-fac87852d85c"},"source":["# https://tinyurl.com/y7dqmy9j\n","\n","from keras.datasets import mnist\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, Flatten\n","\n","#download mnist data and split into train and test sets\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","#reshape data to fit model\n","X_train = X_train.reshape(60000,28,28,1)\n","X_test = X_test.reshape(10000,28,28,1)\n","\n","#one-hot encode target column\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","y_train[0]\n","\n","#create model\n","model = Sequential()\n","#add model layers\n","model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n","model.add(Conv2D(32, kernel_size=3, activation='relu'))\n","\n","model.add(Flatten())\n","model.add(Dense(10, activation='softmax'))\n","\n","#compile model using accuracy to measure model performance\n","# optimizer is used to accelerate gradient descent\n","# Adam accelerates GD using momentum and RMSpeop\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#train the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2)\n","\n","#predict first 4 images in the test set\n","model.predict(X_test[:4])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","Train on 60000 samples, validate on 10000 samples\n","Epoch 1/2\n","60000/60000 [==============================] - 178s 3ms/step - loss: 0.2195 - accuracy: 0.9520 - val_loss: 0.0820 - val_accuracy: 0.9750\n","Epoch 2/2\n","60000/60000 [==============================] - 180s 3ms/step - loss: 0.0699 - accuracy: 0.9785 - val_loss: 0.0980 - val_accuracy: 0.9727\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[1.17635871e-12, 7.55637722e-16, 6.97264989e-12, 1.57320468e-09,\n","        1.15029954e-15, 1.97568849e-13, 4.50498369e-15, 1.00000000e+00,\n","        1.90577931e-09, 2.15209751e-12],\n","       [5.43643486e-09, 3.50316398e-09, 9.99999404e-01, 3.44006490e-11,\n","        9.66579464e-13, 1.01040834e-11, 5.07168068e-07, 9.64732092e-13,\n","        1.39722957e-07, 2.21118290e-14],\n","       [3.24255561e-05, 9.44039524e-01, 7.54457316e-04, 4.23653717e-07,\n","        9.60662364e-05, 1.17718315e-04, 9.71591089e-06, 2.92598588e-06,\n","        5.49466759e-02, 2.14657447e-08],\n","       [9.99994874e-01, 8.14509744e-12, 2.75246236e-07, 4.42323014e-08,\n","        6.63970601e-10, 8.14767773e-08, 1.88084556e-07, 2.48893683e-09,\n","        9.92449259e-07, 3.56639202e-06]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":1}]}]}